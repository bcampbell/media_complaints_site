#!/usr/bin/env python
import os
import sys
import datetime
import re
import base64
import pickle
import urlparse

from django.core.management import setup_environ
import settings

setup_environ(settings)

from django.core.exceptions import ObjectDoesNotExist

from cases.models import *

import MySQLdb
import MySQLdb.cursors

_conn = None

# takes a while to figure out mapping from old case IDs to the new ones, so we'll be saving it out to a file.
CASEMAPFILE = 'case_map'



# map of newspaper names to domains
newspaper_domain_map = {
    'barking & dagenham post': ('www.bdpost.co.uk',),
    'bedfordshire on sunday': ('www.bedsonsunday.com',),
    'birmingham post': ('www.birminghampost.net',),
    'bolton news': ('www.theboltonnews.co.uk',),
    'british medical journal': ('www.bmj.com',),
    'bury times': ('www.burytimes.co.uk',),
    'camden new journal': ('www.camdennewjournal.co.uk',),
    'community care': ('www.communitycare.co.uk',),
    'cornish guardian': ('www.thisiscornwall.co.uk',),
    'croydon guardian': ('www.croydonguardian.co.uk',),
    'daily mail': ('www.dailymail.co.uk',),
    'daily mirror': ('www.mirror.co.uk','www.mirrorfootball.co.uk'),
    'daily star': ('www.dailystar.co.uk',),
    'daily telegraph': ('www.telegraph.co.uk',),
    'evening standard': ('www.thisislondon.co.uk',),
    'financial times': ('search.ft.com',),
    'gazette & herald': ('www.gazetteandherald.co.uk',),
    'greenock telegraph': ('www.greenocktelegraph.co.uk',),
    'hamilton advertiser': ('www.hamiltonadvertiser.co.uk',),
    'harrow observer': ('www.harrowobserver.co.uk',),
    'independent': ('www.independent.co.uk',),
    'isle of wight county press': ('www.iwcp.co.uk',),
    'jewish chronicle': ('www.thejc.com',),
    'lancashire evening post': ('www.lep.co.uk',),
    'lancashire telegraph': ('www.lancashiretelegraph.co.uk',),
    'liverpool echo': ('www.liverpoolecho.co.uk',),
    'mail online': ('www.dailymail.co.uk',),
    'mail on sunday': ('www.dailymail.co.uk','www.mailonsunday.co.uk',),
    'metro': ('www.metro.co.uk',),
    'new scientist': ('www.newscientist.com',),
    'news of the world': ('www.newsoftheworld.co.uk',),
    'nottingham evening post': ('www.thisisnottingham.co.uk',),
    'press & journal': ('www.pressandjournal.co.uk',),
    'sheffield star': ('www.thestar.co.uk',),
    'shropshire star': ('www.shropshirestar.com',),
    'southend echo': ('www.echo-news.co.uk',),
    'stockport express': ('www.stockportexpress.co.uk',),
    'sunday herald permanently deletes report on film produced by nil by mouth, which had provoked a complaint from mrs kim homer, from its archive.': ('www.heraldscotland.com',),
    'sunday mail': ('www.sundaymail.co.uk',),
    'sunday mirror': ('www.mirror.co.uk',),
    'sunday times': ('www.timesonline.co.uk',),
    'sunderland echo': ('www.sunderlandecho.com',),
    'the argus': ('www.theargus.co.uk',),
    'the barrhead news': ('www.barrheadnews.com',),
    'the citizen / gloucestershire echo': ('www.thisisgloucestershire.co.uk',),
    'the daily telegraph': ('www.telegraph.co.uk',),
    'the guardian': ('www.guardian.co.uk',),
    'the independent': ('www.independent.co.uk',),
    'the mail on sunday': ('www.dailymail.co.uk',),
    'the news': ('www.portsmouth.co.uk',),
    'the people': ('www.people.co.uk',),
    'the scotsman': ('heritage.scotsman.com', 'news.scotsman.com', 'thescotsman.scotsman.com',),
    'the sunday times': ('www.timesonline.co.uk',),
    'the sun': ('www.thesun.co.uk',),
    'the times': ('business.timesonline.co.uk','www.timesonline.co.uk',),
    'the times educational supplement': ('www.tes.co.uk',),
    'uttoxeter advertiser': ('www.uttoxeteradvertiser.co.uk',),
    'wiltshire times': ('archive.wiltshiretimes.co.uk',),
    'worksop guardian': ('www.worksopguardian.co.uk',),
}


def get_article_id( old ):
    """ get pcc article id from urls """
    urls = get_urls(old, 'pcc.org.uk' )
    if len(urls)==0:
        return None
    assert len(urls)==1
    url = urls[0]

    id_pat = re.compile( 'article=(.+?)(?:http://.*)?$' )
    m = id_pat.search( url )
    if m is None:
#        print "%s: WARNING: couldn't extract article id from url '%s'" %( old['id'],url )
        return None

    encoded_id = m.group(1)
    if not encoded_id.endswith('=='):
        # missing padding
        assert len(encoded_id) == 6
        encoded_id = encoded_id + "=="
    article_id = int( base64.urlsafe_b64decode( encoded_id ) )
    return article_id





def find_cases( old ):
    """ find case(s) in new db, corresponding to the ones in the old one """
    cases = []

    method = "not found"

    legacy_id = get_article_id( old )
    #print "legacy_id: ",legacy_id
    if legacy_id is not None:
        cases = Case.objects.filter( legacy_id=legacy_id )

    if not cases:
        # use complaint text to resolve:
        if old['complaint'].strip() == '':
            pass
#            print "%s: blank" %(old['id'],)
        else:
#            print "using complaint"
#            print old['complaint'].split()
            cases = Case.objects.filter( complaint__icontains=old['complaint'].strip() )

    if not cases:
        # try using complainant and complaint_about:
        cases = Case.objects.filter(
            complainants__name__iexact=old['complainant'].strip(),
            defendants__name__iexact=old['complaint_about'].strip() )
        if len(cases)==1:
            pass
#            print "%s: matched using complainant/defendant" % (old['id'],)
        elif len(cases)>1:
            # if ambiguous, forget it.
#            print "%s: matched using complainant/defendant but ambiguous, so discarding" % (old['id'],)
            cases = []
    #print old['id'], method

    return [ case.pk for case in cases ]


def get_urls(old, match=None):
    """ get list of urls attached to story in old db """
    urls = []
    for f in ( 'url_of_story', 'url_of_story_2' ):
        url = old[f]
        if url is None:
            continue
        url = url.strip()
        if url=='':
            continue
        if (match is None) or (match in url):
            urls.append( url )
    return urls



def build_case_map():
    """ build mapping of old db id to new db id(s) """
    print "Building case map..."
    cursor = _conn.cursor()
    cursor.execute ("SELECT id,complaint,complainant,complaint_about,url_of_story,url_of_story_2 FROM complaints WHERE complaint_body='PCC'")

    # mapping from old db id to new cases in django db
    case_map = {}
    reverse_map = {}

    found = 0
    total = 0
    for row in cursor:
        cases = find_cases( row )
#        if len(cases)>1:
#            print "%s: WARNING: multiple cases" % (row['id'],)

        assert row['id'] not in case_map
        case_map[ row['id'] ] = cases
        if len(cases)>0:
            found=found+1
#            print row['id'] , "->", cases
        else:
            print "http://vernon.mediastandardstrust.org/~ben/complaints/index.php?action=view&pk=%s" % (row['id'],)

        total=total+1

    print found,"/",total
    cursor.close()
    return case_map




def get_related_articles( old, publication_name ):
    urls = get_urls(old)

    # get rid of pcc links
    urls = [ url for url in urls if 'pcc.org.uk' not in url ]
    urls = [ url for url in urls if url.lower()!="not available" ]
    urls = [ url for url in urls if url.lower()!="removed" ]


    offending = []
    related = []
    for url in urls:
        o = urlparse.urlparse( url )
        domain = o[1].lower()
        if domain=='':
            continue
        if domain in newspaper_domain_map.get( publication_name.lower(), () ):
            offending.append( {'url':url, 'publication': publication_name } )
        else:
            related.append( {'url':url} )


    return offending, related


def add_offending_articles( case, articles ):
    for art in articles:
        if case.offending_articles.filter( url=art['url'] ).count() > 0:
            continue
        else:
            pub = case.defendants.all()[0]
            a = Article( url=art['url'], publication=pub )
            a.save()
            case.offending_articles.add( a )
            print "case %d: add offending: %s (%s)" % (case.pk, a.url, a.publication.name)


def add_related_articles( case, articles ):
    for art in articles:
        if case.related_links.filter( url=art['url'] ).count() > 0:
            continue
        else:
            a = Article( url=art['url'] )
            a.save()
            case.related_links.add( a )
            print "case %d: add related: %s" % (case.pk, a.url)



def get_complaint_keywords( old ):

    raw = old['complaint_keyword']
    if old['complaint_keyword_2']:
        raw = raw + ',' + old['complaint_keyword_2']
    if old['complaint_keyword_3']:
        raw = raw + ',' + old['complaint_keyword_3']

    fixups = {
            'Dicrimination': 'Discrimination',
            'Deat': 'Death',
            'Factual Error': 'Factual error',
            'Inaccuray': 'Inaccuracy',
            'Photogragph': 'Photograph',
            'Photographs': 'Photograph',
            'Photography': 'Photograph',
            'Relationship': 'Relationships',
            'Right to Reply': 'Right to reply',
            }
    tags = raw.split(',')
    tags = [ t.strip() for t in tags ]
    tags = [ t for t in tags if t ]
    tags = [fixups.get(t,t) for t in tags]
    tags = list( set(tags) )    #uniquify
    return tags


def add_tags( case, tags ):
    for t in tags:
        tag_obj, created = Tag.objects.get_or_create( name__iexact=t )
        tag_obj.save()
        case.tags.add( tag_obj )





def get_related_cases( old,case_map ):
    related = set()

    raw = old['related_complaint']

    # "382-8" => "382-388"
    raw = re.sub( r'\b(\d+)(\d)-(\d)\b', r'\1\2-\1\3', raw )

    # 1258-1261 => "1258,1259,1260,1261"
    def repl( m ):
        return ','.join( [str(i) for i in range( int(m.group(1)),int(m.group(2))+1 ) ] )
    raw = re.sub( r'\b(\d+)-(\d+)\b', repl, raw )

#    if raw:
#        print old['id'], "---->", raw

    for r in raw.split(','):
        r=r.strip()
        if r == '':
            continue
        old_id = int(r)
        new_id = case_map[old_id]
        related.update(new_id)
#    if raw:
#        print old['id'], "====>", related
    return related



def add_related_cases( case, related_ids ):

    changed = False

    for id in related_ids:
        if id==case.pk or case.related_cases.filter(pk=id).count()>0:
            continue
        else:
            print "%s: added related case %s" % (case.id, id )
            case.related_cases.add( Case.objects.get(pk=id) )
            changed = True
    return changed



def main():
    global _conn

    _conn = MySQLdb.connect( host = "",
                            user = "root",
                            passwd = "",
                            db = "complaints",
                            cursorclass=MySQLdb.cursors.DictCursor )

    if len(sys.argv)>1 and sys.argv[1] == 'build_case_map':
        case_map = build_case_map()
        outfile = open( CASEMAPFILE, 'wb' )
        pickle.dump( case_map, outfile )
        outfile.close()
        print "wrote ", CASEMAPFILE
        sys.exit(0)

    f = open( CASEMAPFILE, 'rb' )
    case_map = pickle.load( f )
    f.close()

    print "slurping data over now..."
    cursor = _conn.cursor()
    cursor.execute ("SELECT * FROM complaints WHERE complaint_body='PCC'")
    for row in cursor:

        tags = get_complaint_keywords( row )
        related = get_related_cases( row,case_map)


#        print row['id']
        cases = case_map[row['id']]
        for case_id in cases:
            case = Case.objects.get( pk=case_id )
            if tags and case.tags.count()==0:
                print "%s: case %d: add tags" % (row['id'],case.pk,)
                add_tags( case, tags )
            if row['complaint_title'].strip() and case.summary == '':
                case.summary = row['complaint_title'].strip()
                print "%s: case %d: set summary" % (row['id'],case.pk,)


            # add split cases as related cases:

            all_related = related.union( set([ c for c in cases if c != case_id ]) )
            if all_related:
#                print "all_related: ", all_related
                add_related_cases( case, all_related )

            offending_arts,related_arts = get_related_articles( row, case.defendants.all()[0].name )

            add_offending_articles( case, offending_arts )
            add_related_articles( case, related_arts )


            case.save()

    cursor.close()

    _conn.close()

main()

