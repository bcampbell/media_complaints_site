#!/usr/bin/env python
#
# import complaints from the PCC website, using the scraperwiki pcc-decisions scraper
#
import os
import sys
import datetime
import urllib
import urllib2
import json
import pprint
import dateutil.parser
import re

from django.core.management import setup_environ
import settings

setup_environ(settings)

from django.core.exceptions import ObjectDoesNotExist

from cases.models import *


_conn = None


# cheesiness for local load
f = open( "../pcc_scraper/output.json" )
json_data = f.read()
f.close()
incoming_data = json.loads( json_data )
#fudge
for c in incoming_data:
    c['date_scraped'] = datetime.datetime.now().isoformat( ' ' )

def slurp_batch( offset, limit ):
    return incoming_data[offset:offset+limit]

def OLD_slurp_batch( offset, limit ):

    url = "http://api.scraperwiki.com/api/1.0/datastore/getdata"

    params = { 'format': 'json',
        'name': 'pcc-decisions',
        'limit': str(limit),
        'offset': str(offset) }
    url = url + "?" + urllib.urlencode( params )
    f = urllib2.urlopen( url )
    json_data = f.read()
#    print json_data

#    print "\n\n----------\n\n"
    out = json.loads( json_data )
    return out


def load_pcc_clauses():
    """ make sure all the PCC codes are in the system """
    pcc_clauses = [
        (1, u'Accuracy'),
        (2, u'Opportunity to reply'),
        (3, u'Privacy'),
        (4, u'Harassment'),
        (5, u'Intrusion into grief or shock'),
        (6, u'Children'),
        (7, u'Children in sex cases'),
        (8, u'Hospitals'),
        (9, u'Reporting of Crime'),
        (10, u'Clandestine devices and subterfuge'),
        (11, u'Victims of sexual assault'),
        (12, u'Discrimination'),
        (13, u'Financial journalism'),
        (14, u'Confidential sources'),
        (15, u'Witness payments in criminal trials'),
        (16, u'Payment to criminals'),
    ]

    for d in pcc_clauses:
        clause,created = Clause.objects.get_or_create( ident=d[0], prettyname=d[1] )
        if created:
            clause.save()


def process( c ):
    # don't add ones we've already got
    existing = Case.objects.filter( legacy_id = str(c['id']) )
    if existing:
        return

    publications = re.split( r'\s*/\s*', c['Publication'] )
    if len( publications ) > 1:
        print "splitting %s ('%s')" % ( c['url'], c['Publication'] )
    new_cases = []
    for pub in publications:
        c2 = c
        c2['Publication'] = pub
        new_cases.append( add_case(c2) )

    # if multiple cases, make them related:
    for case in new_cases:
        for other in new_cases:
            if other.pk == case.pk:
                continue
            case.related_cases.add( other )



def add_case( c ):
    assert( 'url' in c )


    # incoming fields:
    #
    # Date_Published    u'18/11/08',
    # Decision          u'Upheld', (if adjudicated)
    # Adjudication      text (if adjudicated)
    # Resolution        text (if resolved)
    # Publication       u'News of the World',
    # Report            eg "78"
    # date_scraped      eg u'2010-10-27 13:23:08' 
    # id                article_id from pcc db (decoded from base64 url)
    # url
    # Complainant_Name
    # Clauses_Noted
    # Complaint         text summary

    if c['Clauses_Noted'].strip().lower() == 'none':
        c['Clauses_Noted'] = u''
    date_published = dateutil.parser.parse( c['Date_Published'], dayfirst=True )
    assert( date_published is not None )


    # create the PCC if it doesn't exist ;-)
    the_pcc, created = Entity.objects.get_or_create( name='PCC', kind='c' )
    if created:
        the_pcc.save()


    outcome = None
    if 'Resolution' in c:
        outcome = u'Resolved'
    elif 'Decision' in c:
        assert 'Adjudication' in c
        outcome = c['Decision']
    if outcome is None:
        raise Exception( "%s: couldn't decide outcome" % (c['url']) )

    # create the new case
    case = Case()

    case.checked = False
    case.legacy_id = str( c['id'] )
    case.date_of_decision = date_published
    case.url_of_complaint = c['url']
    case.complaint = c['Complaint']
    case.complaint_body = the_pcc

    case.date_scraped = dateutil.parser.parse( c['date_scraped'], dayfirst=True )

    if 'Report' in c:
        case.report = c['Report']

    case.title = u"%s v %s" % (c['Complainant_Name'],c['Publication'] )

    case.outcome, created = Outcome.objects.get_or_create( name=outcome )
    if created:
        case.outcome.save()

    # clauses
    # check and store them for later adding (m2m)
    pcc_clause_objs = []
    clause_idents = set( [ clause.strip() for clause in c['Clauses_Noted'].split(',') ] )
    for ident in clause_idents:
        if ident == u'':
            continue
        clause, created = Clause.objects.get_or_create( ident=ident )
        if created:
            clause.save()
        pcc_clause_objs.append( clause )


    # complainant
    # could be more clever and try and identify multiple complaintants, but that's a can of worms:
    #  "Mr Andrew Rowell, Dr Peter Moore, and Dr Simon Lewis"
    #  "Lesbian, Gay, Bisexual, Transgender (LGBT) Youth Scotland"
    #  "Mr Peter Handy, the Executive Director of Kemple View Hospital"
    #  "Mrs Pauline Dunn on behalf of her niece, Mrs Wendy Barlow"
    #    etc...
    complainant_objs = []
    complainant,created = Entity.objects.get_or_create( name=c['Complainant_Name'], kind='p' )
    complainant_objs.append( complainant )
    if created:
        complainant.save()

    # who they are complaining about
    # nice and easy to identify multiples:

    publications = re.split( r'\s*/\s*', c['Publication'] )

    # we now split up complaints against multiple publications
    assert( len(publications) == 1 )
#    if len( publications ) >1:
#        print "multiple publications: ", c['Publication'], "=>", publications

    defendant_objs = []
    for publication in publications:
        about,created = Entity.objects.get_or_create( name=publication, kind='m' )
        defendant_objs.append( about )
        if created:
            about.save()

    # need to save it before we can add the m2m fields
    case.save()

    case.clauses.add( *pcc_clause_objs )
#    case.tags.add( *keyword_objs )
    case.complainants.add( *complainant_objs )
    case.defendants.add( *defendant_objs )


    # now add the details to the case
    if "Resolution" in c:
        resolution = Detail( content=c['Resolution'], kind='resolution', case=case )
        resolution.save()

    if "Adjudication" in c:
        adj = Detail( content=c['Adjudication'], kind='adjudication', case=case )
        adj.save()


    print "added %d" %  (case.pk )

    return case



def main():
    load_pcc_clauses()

    errcnt = 0
    offset = 0
    limit = 500
    while True:
        batch = slurp_batch( offset, limit )
        print "got %d" % ( len(batch) )
        if len( batch ) == 0:
            break

        for complaint in batch:
            c = complaint

            res = "nores"
            if "Resolution" in c:
                res = "  res"
            adj = "noadj"
            if "Adjudication" in c:
                adj = "  adj"
            desc = 'nodesc'
            if "Decision" in c:
                desc = c['Decision']

#            print "%s %s %s" % ( res, desc, adj )

            try:
                process( complaint )
            except Exception, e:
                print "\n\n---------"
                print "ERROR: ", e
                print "---------"
                pprint.pprint( complaint )
                print "---------\n\n"
                errcnt = errcnt+1
                if errcnt>100:
                    print "TOO MANY ERRORS"
                    raise

        offset = offset + limit

main()

