#!/usr/bin/env python
#
# import complaints from the PCC website, via the scraperwiki pcc-decisions-mark-3 scraper
#
import os
import sys
import datetime
import urllib
import urllib2
import json
import pprint
import dateutil.parser
import re
import optparse
import logging

from django.core.management import setup_environ
import settings

setup_environ(settings)

from django.core.exceptions import ObjectDoesNotExist

from cases.models import *


_conn = None
_opts = None

# cheesiness for local load
#f = open( "../pcc_scraper/output.json" )
#json_data = f.read()
#f.close()
#incoming_data = json.loads( json_data )
#fudge
#for c in incoming_data:
#    c['date_scraped'] = datetime.datetime.now().isoformat( ' ' )
#
#def slurp_batch( offset, limit ):
#    return incoming_data[offset:offset+limit]
#

def slurp_batch( offset, limit ):

#    url = "http://api.scraperwiki.com/api/1.0/datastore/getdata"
#    params = { 'format': 'json',
#        'name': 'pcc-decisions-mark-3',
#        'limit': str(limit),
#        'offset': str(offset) }
#    url = url + "?" + urllib.urlencode( params )

    url = "http://api.scraperwiki.com/api/1.0/datastore/sqlite?format=jsondict&name=pcc-decisions-mark-3&query=select%20*%20from%20swdata%20limit%20" + str(limit) + "%20offset%20" + str(offset)

    f = urllib2.urlopen( url )
    json_data = f.read()
#    print json_data

#    print "\n\n----------\n\n"
    out = json.loads( json_data )
    return out


def load_pcc_clauses():
    """ make sure all the PCC codes are in the system """
    pcc_clauses = [
        (1, u'Accuracy'),
        (2, u'Opportunity to reply'),
        (3, u'Privacy'),
        (4, u'Harassment'),
        (5, u'Intrusion into grief or shock'),
        (6, u'Children'),
        (7, u'Children in sex cases'),
        (8, u'Hospitals'),
        (9, u'Reporting of Crime'),
        (10, u'Clandestine devices and subterfuge'),
        (11, u'Victims of sexual assault'),
        (12, u'Discrimination'),
        (13, u'Financial journalism'),
        (14, u'Confidential sources'),
        (15, u'Witness payments in criminal trials'),
        (16, u'Payment to criminals'),
    ]

    for d in pcc_clauses:
        clause,created = Clause.objects.get_or_create( ident=d[0], prettyname=d[1] )
        if created:
            clause.save()



def add_case( c ):
    assert( 'url' in c )

    c['complainant_name'] = tidy_complainant_name( c['complainant_name'], c['publication'] )

    case_title = u"%s v %s" % (c['complainant_name'],c['publication'] )

    logging.info( "add '%s' %s - %s" % ( case_title, c['date_published'], c['url'] ) )

    # incoming fields:
    #
    # Date_Published    u'18/11/08',
    # Decision          u'Upheld', (if adjudicated)
    # Adjudication      text (if adjudicated)
    # Resolution        text (if resolved)
    # Publication       u'News of the World',
    # Report            eg "78"
    # date_scraped      eg u'2010-10-27 13:23:08' 
    # id                article_id from pcc db (decoded from base64 url)
    # url
    # Complainant_Name
    # Clauses_Noted
    # Complaint         text summary

    if c['clauses_noted'].strip().lower() == 'none':
        c['clauses_noted'] = u''
    date_published = dateutil.parser.parse( c['date_published'], dayfirst=True )
    assert( date_published is not None )

    if date_published > datetime.datetime.now():
        logging.warning( "skip case with future date (%s) %s" % (c['date_published'],c['url']))
        return


    if _opts.dry_run:
        return

    # create the PCC if it doesn't exist ;-)
    the_pcc, created = Entity.objects.get_or_create( name='PCC', kind='c' )
    if created:
        the_pcc.save()


    outcome = None
    if c.get('resolution',''):
        outcome = u'Resolved'
    elif c.get('decision',''):
        assert 'adjudication' in c
        assert c['adjudication']
        outcome = c['decision']
    if outcome is None:
        raise Exception( "%s: couldn't decide outcome" % (c['url']) )

    # create the new case
    case = Case()

    case.checked = False
    case.legacy_id = str( c['id'] )
    case.date_of_decision = date_published
    case.url_of_complaint = c['url']
    case.complaint = c['complaint']
    case.complaint_body = the_pcc

#    case.date_scraped = dateutil.parser.parse( c['date_scraped'], dayfirst=True )
    case.date_scraped = None

    if c.get('report',None):
        case.report = c['report']

    case.title = case_title

    case.outcome, created = Outcome.objects.get_or_create( name=outcome )
    if created:
        case.outcome.save()

    # clauses
    # check and store them for later adding (m2m)
    pcc_clause_objs = []
    clause_idents = set( [ clause.strip() for clause in c['clauses_noted'].split(',') ] )
    for ident in clause_idents:
        if ident == u'':
            continue
        clause, created = Clause.objects.get_or_create( ident=ident )
        if created:
            clause.save()
        pcc_clause_objs.append( clause )


    # complainant
    # could be more clever and try and identify multiple complaintants, but that's a can of worms:
    #  "Mr Andrew Rowell, Dr Peter Moore, and Dr Simon Lewis"
    #  "Lesbian, Gay, Bisexual, Transgender (LGBT) Youth Scotland"
    #  "Mr Peter Handy, the Executive Director of Kemple View Hospital"
    #  "Mrs Pauline Dunn on behalf of her niece, Mrs Wendy Barlow"
    #    etc...
    complainant_objs = []
    complainant,created = Entity.objects.get_or_create( name=c['complainant_name'], kind='p' )
    complainant_objs.append( complainant )
    if created:
        complainant.save()

    # who they are complaining about
    # nice and easy to identify multiples:

    publications = re.split( r'\s*/\s*', c['publication'] )
    if len( publications ) >1:
        logging.info( "split multiple publications: %s=>%s" % ( c['publication'], publications ) )
    defendant_objs = []
    for publication in publications:
        about,created = Entity.objects.get_or_create( name=publication, kind='m' )
        defendant_objs.append( about )
        if created:
            about.save()

    # need to save it before we can add the m2m fields
    case.save()

    case.clauses.add( *pcc_clause_objs )
#    case.tags.add( *keyword_objs )
    case.complainants.add( *complainant_objs )
    case.defendants.add( *defendant_objs )


    # now add the details to the case
    if c.get("resolution",''):
        resolution = Detail( content=c['resolution'], kind='resolution', case=case )
        resolution.save()

    if c.get("adjudication", '' ):
        adj = Detail( content=c['adjudication'], kind='adjudication', case=case )
        adj.save()

    return case


def tidy_complainant_name(name,publication):
    """ handle cases where complainant name has whole case title """

    oldname = name

    name = name.replace(u' v ' + publication, u'')
    borkpat =  re.compile( r'^\s*(Resolved|Adjudication|Adjudicated)\s*-\s*', re.IGNORECASE )
    name = borkpat.sub( u'', name )

    if name != oldname:
        logging.warn( "TIDY complainant name '%s' => '%s'" % (oldname, name) )
    return name



def main():
    global _opts

    parser = optparse.OptionParser()
    parser.add_option('-d', '--dry-run', action='store_true', dest='dry_run', help="Don't modify the database" )
    parser.add_option("-v", "--verbose", dest="verbose", action="store_true" )
    (_opts, args) = parser.parse_args(sys.argv)

    # set up logging
    lvl = logging.WARNING
    if _opts.verbose:
        lvl = logging.INFO
    logging.basicConfig(level=lvl, format='%(message)s')


    if not _opts.dry_run:
        load_pcc_clauses()
    else:
        logging.warning( 'performing dry run' )

    errcnt = 0
    offset = 0
    limit = 500
    new_cnt = 0
    while True:
        logging.info( 'fetching %s-%s' % (offset, offset+limit) )
        batch = slurp_batch( offset, limit )
        if len( batch ) == 0:
            break

        for complaint in batch:
            c = complaint

#            res = "nores"
#            if "Resolution" in c:
#                res = "  res"
#            adj = "noadj"
#            if "Adjudication" in c:
#                adj = "  adj"
#            desc = 'nodesc'
#            if "Decision" in c:
#                desc = c['Decision']
#            print "%s %s %s" % ( res, desc, adj )

            try:

                # don't add ones we've already got
                existing = Case.objects.filter( legacy_id = str(c['id']) )
                if existing:
                    # TODO: check for changes?
                    continue
                if 'publication' not in c:
                    logging.warning( "skip blank record %s" % ( c['url'], ) )
                    continue
                new_cnt += 1
                add_case( c )

            except Exception, e:
                print "\n\n---------"
                print "ERROR: ", e
                print "---------"
                pprint.pprint( complaint )
                print "---------\n\n"
                errcnt = errcnt+1
                if errcnt>100:
                    logging.critical( "too many errors" )
                    raise

        offset = offset + limit
    logging.info( "done - %d new cases" % new_cnt )


if __name__ == '__main__':
    main()

